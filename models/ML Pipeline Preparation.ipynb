{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\brian.meki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brian.meki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pysftp\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import os \n",
    "from sqlalchemy import create_engine \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.stem import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///MessagesDump.db')\n",
    "df = pd.read_sql(\"SELECT * FROM D_Messages\", con=engine)\n",
    "\n",
    "# Close the connection\n",
    "engine.dispose()\n",
    "\n",
    "# clean data by replacing NaNs with 0\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_fun(text_data):\n",
    "    # Lowercasing\n",
    "    text_data = text_data.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text_data)\n",
    "\n",
    "    # Removing Punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # Removing Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokenized_list = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Cleaning\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for tok in tokenized_list:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "        \n",
    "    # Return results\n",
    "    return clean_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update messages data frame \n",
    "df['tokenized_text'] = df['message'].apply(tokenize_fun)\n",
    "df.head();\n",
    "\n",
    "# Define predicted and predictor variables\n",
    "X = df['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "Y = df.iloc[:, 4:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to join the tokenized words back into strings\n",
    "def identity_tokenizer(tokens):\n",
    "    return tokens\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer = identity_tokenizer,lowercase=True, stop_words='english')),  # Vectorize tokenized text data using TF-IDF\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))  # Multi-output Random Forest Classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brian.meki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\brian.meki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit the pipeline on the training data\n",
    "mdl = pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Format predicted output\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.columns = y_train.columns\n",
    "predictions_df.head();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.9308779149519891\n",
      "Mean Recall: 0.9308779149519891\n",
      "Mean F1 Score: 0.906960719927145\n",
      "Mean Precision: 0.657283950617284\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "# Create an empty list to store accuracy values\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each output separately\n",
    "for i in range(y_test.shape[1]): \n",
    "    accuracy_i = accuracy_score(y_test.iloc[:, i], predictions_df.iloc[:, i])\n",
    "    accuracies.append(accuracy_i)\n",
    "\n",
    "# Now, accuracies contains the accuracy values for each output\n",
    "# You can access the accuracies using accuracies[index]\n",
    "\n",
    "# Calculate mean accuracy across all outputs\n",
    "mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "\n",
    "# Recall\n",
    "# Create an empty list to store recall values\n",
    "recalls = []\n",
    "\n",
    "# Iterate over each output separately\n",
    "for i in range(y_test.shape[1]):\n",
    "    recalls_i = recall_score(y_test.iloc[:, i], predictions_df.iloc[:, i], average='weighted', zero_division=0)\n",
    "    recalls.append(recalls_i)\n",
    "     \n",
    "# Calculate mean recalls across all outputs\n",
    "mean_recall = sum(recalls) / len(recalls)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "\n",
    "# F1 Score\n",
    "# Create an empty list to store f1_score values\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over each output separately\n",
    "for i in range(y_test.shape[1]):\n",
    "    f1_score_i = f1_score(y_test.iloc[:, i], predictions_df.iloc[:, i], average='weighted', zero_division=0)\n",
    "    f1_scores.append(f1_score_i)\n",
    "     \n",
    "# Calculate mean recalls across all outputs\n",
    "mean_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "print(\"Mean F1 Score:\", mean_f1_score)\n",
    "\n",
    "# Precision\n",
    "# Create an empty list to store accuracy values\n",
    "precisions = []\n",
    "\n",
    "# Iterate over each output separately\n",
    "for i in range(y_test.shape[1]):\n",
    "    class_report_i = precision_score(y_test.iloc[:, i], predictions_df.iloc[:, i], average='weighted', zero_division=0)\n",
    "    precisions.append(accuracy_i)\n",
    "\n",
    "# Calculate mean accuracy across all outputs\n",
    "mean_precision = sum(precisions) / len(precisions)\n",
    "print(\"Mean Precision:\", mean_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\brian.meki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brian.meki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\brian.meki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__estimator__max_depth': None, 'clf__estimator__n_estimators': 50, 'tfidf__max_features': 5000}\n",
      "Best Mean Cross-Validation Accuracy: nan\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Download NLTK resources if not already present\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to tokenize text using NLTK\n",
    "def nltk_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and Random Forest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=nltk_tokenizer, lowercase=True, stop_words='english')),  \n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))  \n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000, None],  # Adjust as needed\n",
    "    'clf__estimator__n_estimators': [50, 100, 200],  # Adjust as needed\n",
    "    'clf__estimator__max_depth': [None, 10, 20],  # Adjust as needed\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=2, n_jobs=-1, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by grid search\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best cross-validation score\n",
    "print(\"Best Mean Cross-Validation Accuracy:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your (Improved) model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.9278052126200276\n",
      "Mean Recall: 0.9278052126200276\n",
      "Mean F1 Score: 0.9093950501882146\n",
      "Mean Precision: 0.6474074074074071\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "predictions = grid_search.predict(X_test)\n",
    "\n",
    "# Format predicted output\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.columns = y_train.columns\n",
    "predictions_df.head();\n",
    "\n",
    "# Accuracy\n",
    "# Create an empty list to store accuracy values\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each output separately\n",
    "for i in range(y_test.shape[1]): \n",
    "    accuracy_i = accuracy_score(y_test.iloc[:, i], predictions_df.iloc[:, i])\n",
    "    accuracies.append(accuracy_i)\n",
    "\n",
    "# Now, accuracies contains the accuracy values for each output\n",
    "# You can access the accuracies using accuracies[index]\n",
    "\n",
    "# Calculate mean accuracy across all outputs\n",
    "mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "\n",
    "# Recall\n",
    "# Create an empty list to store recall values\n",
    "recalls = []\n",
    "\n",
    "# Iterate over each output separately\n",
    "for i in range(y_test.shape[1]):\n",
    "    recalls_i = recall_score(y_test.iloc[:, i], predictions_df.iloc[:, i], average='weighted', zero_division=0)\n",
    "    recalls.append(recalls_i)\n",
    "     \n",
    "# Calculate mean recalls across all outputs\n",
    "mean_recall = sum(recalls) / len(recalls)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "\n",
    "# F1 Score\n",
    "# Create an empty list to store f1_score values\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over each output separately\n",
    "for i in range(y_test.shape[1]):\n",
    "    f1_score_i = f1_score(y_test.iloc[:, i], predictions_df.iloc[:, i], average='weighted', zero_division=0)\n",
    "    f1_scores.append(f1_score_i)\n",
    "     \n",
    "# Calculate mean recalls across all outputs\n",
    "mean_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "print(\"Mean F1 Score:\", mean_f1_score)\n",
    "\n",
    "# Precision\n",
    "# Create an empty list to store accuracy values\n",
    "precisions = []\n",
    "\n",
    "# Iterate over each output separately\n",
    "for i in range(y_test.shape[1]):\n",
    "    class_report_i = precision_score(y_test.iloc[:, i], predictions_df.iloc[:, i], average='weighted', zero_division=0)\n",
    "    precisions.append(accuracy_i)\n",
    "\n",
    "# Calculate mean accuracy across all outputs\n",
    "mean_precision = sum(precisions) / len(precisions)\n",
    "print(\"Mean Precision:\", mean_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is performant to my present abilities and time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
