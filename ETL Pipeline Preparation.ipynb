{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline Preparation\n",
    "Follow the instructions below to help you create your ETL pipeline.\n",
    "### 1. Import libraries and load datasets.\n",
    "- Import Python libraries\n",
    "- Load `messages.csv` into a dataframe and inspect the first few lines.\n",
    "- Load `categories.csv` into a dataframe and inspect the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pysftp # library provides sftp connectivity tools\n",
    "import pandas as pd # library provides mathematical suit \n",
    "from datetime import datetime as dt #library allows for manipulation of dates and time\n",
    "import os # library allows for detection and manipulation of file paths / directories\n",
    "from sqlalchemy import create_engine # library allows for creation of sql engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26180    id                                            message  \\\n",
      "0   2  Weather update - a cold front from Cuba that c...   \n",
      "1   7            Is the Hurricane over or is it not over   \n",
      "2   8                    Looking for someone but no name   \n",
      "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
      "4  12  says: west side of Haiti, rest of the country ...   \n",
      "\n",
      "                                            original   genre  \n",
      "0  Un front froid se retrouve sur Cuba ce matin. ...  direct  \n",
      "1                 Cyclone nan fini osinon li pa fini  direct  \n",
      "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct  \n",
      "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct  \n",
      "4  facade ouest d Haiti et le reste du pays aujou...  direct  \n"
     ]
    }
   ],
   "source": [
    "# load messages dataset\n",
    "messages = pd.read_csv('C:/Users/brian.meki/Desktop/Udacity/Data/disaster_messages.csv')\n",
    "\n",
    "# clean data by removing duplicated rows\n",
    "messages = messages.drop_duplicates('id')\n",
    "print(messages.shape[0],\n",
    "messages.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26180    id                                         categories\n",
      "0   2  related-1;request-0;offer-0;aid_related-0;medi...\n",
      "1   7  related-1;request-0;offer-0;aid_related-1;medi...\n",
      "2   8  related-1;request-0;offer-0;aid_related-0;medi...\n",
      "3   9  related-1;request-1;offer-0;aid_related-1;medi...\n",
      "4  12  related-1;request-0;offer-0;aid_related-0;medi...\n"
     ]
    }
   ],
   "source": [
    "# load categories dataset\n",
    "categories =  pd.read_csv('C:/Users/brian.meki/Desktop/Udacity/Data/disaster_categories.csv')\n",
    "\n",
    "# clean data by removing duplicated rows\n",
    "categories = categories.drop_duplicates('id')\n",
    "\n",
    "print(categories.shape[0],\n",
    "categories.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge datasets.\n",
    "- Merge the messages and categories datasets using the common id\n",
    "- Assign this combined dataset to `df`, which will be cleaned in the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26180\n"
     ]
    }
   ],
   "source": [
    "# merge datasets\n",
    "df_Initial = pd.merge(messages, categories, on='id', how='inner')\n",
    "\n",
    "print(df_Initial.duplicated('id').sum(),\n",
    "      df_Initial.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split `categories` into separate category columns.\n",
    "- Split the values in the `categories` column on the `;` character so that each value becomes a separate column. You'll find [this method](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.Series.str.split.html) very helpful! Make sure to set `expand=True`.\n",
    "- Use the first row of categories dataframe to create column names for the categories data.\n",
    "- Rename columns of `categories` with new column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the 36 individual category columns\n",
    "categories = categories['categories'].str.split(';', expand=True)\n",
    "categories.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n"
     ]
    }
   ],
   "source": [
    "# select the first row of the categories dataframe\n",
    "row = categories.iloc[0]\n",
    "\n",
    "# use this row to extract a list of new column names for categories.\n",
    "# one way is to apply a lambda function that takes everything \n",
    "# up to the second to last character of each string with slicing\n",
    "category_colnames = [value[:-2] for value in row]\n",
    "print(category_colnames);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26180"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename the columns of `categories`\n",
    "categories.columns = category_colnames\n",
    "categories.head();\n",
    "categories.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert category values to just numbers 0 or 1.\n",
    "- Iterate through the category columns in df to keep only the last character of each string (the 1 or 0). For example, `related-0` becomes `0`, `related-1` becomes `1`. Convert the string to a numeric value.\n",
    "- You can perform [normal string actions on Pandas Series](https://pandas.pydata.org/pandas-docs/stable/text.html#indexing-with-str), like indexing, by including `.str` after the Series. You may need to first convert the Series to be of type string, which you can do with `astype(str)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categories:\n",
    "    # set each value to be the last character of the string\n",
    "    categories[column] = [value[-1] for value in categories[column]]\n",
    "\n",
    "    # convert column from string to numeric\n",
    "    categories[column] = pd.to_numeric(categories[column])\n",
    "categories.head();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Replace `categories` column in `df` with new category columns.\n",
    "- Drop the categories column from the df dataframe since it is no longer needed.\n",
    "- Concatenate df and categories data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original categories column from `df`\n",
    "df = df_Initial.drop(columns=['categories'])\n",
    "df.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26248      id                                            message  \\\n",
      "0   2.0  Weather update - a cold front from Cuba that c...   \n",
      "1   7.0            Is the Hurricane over or is it not over   \n",
      "2   8.0                    Looking for someone but no name   \n",
      "3   9.0  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
      "4  12.0  says: west side of Haiti, rest of the country ...   \n",
      "\n",
      "                                            original   genre  related  \\\n",
      "0  Un front froid se retrouve sur Cuba ce matin. ...  direct      1.0   \n",
      "1                 Cyclone nan fini osinon li pa fini  direct      1.0   \n",
      "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct      1.0   \n",
      "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct      1.0   \n",
      "4  facade ouest d Haiti et le reste du pays aujou...  direct      1.0   \n",
      "\n",
      "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
      "0      0.0    0.0          0.0           0.0               0.0  ...   \n",
      "1      0.0    0.0          1.0           0.0               0.0  ...   \n",
      "2      0.0    0.0          0.0           0.0               0.0  ...   \n",
      "3      1.0    0.0          1.0           0.0               1.0  ...   \n",
      "4      0.0    0.0          0.0           0.0               0.0  ...   \n",
      "\n",
      "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
      "0          0.0                   0.0              0.0     0.0    0.0   0.0   \n",
      "1          0.0                   0.0              1.0     0.0    1.0   0.0   \n",
      "2          0.0                   0.0              0.0     0.0    0.0   0.0   \n",
      "3          0.0                   0.0              0.0     0.0    0.0   0.0   \n",
      "4          0.0                   0.0              0.0     0.0    0.0   0.0   \n",
      "\n",
      "   earthquake  cold  other_weather  direct_report  \n",
      "0         0.0   0.0            0.0            0.0  \n",
      "1         0.0   0.0            0.0            0.0  \n",
      "2         0.0   0.0            0.0            0.0  \n",
      "3         0.0   0.0            0.0            0.0  \n",
      "4         0.0   0.0            0.0            0.0  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# concatenate the original dataframe with the new `categories` dataframe\n",
    "df = pd.concat([df, categories], axis=1)\n",
    "print(df.shape[0],df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove duplicates.\n",
    "- Check how many duplicates are in this dataset.\n",
    "- Drop the duplicates.\n",
    "- Confirm duplicates were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 41\n"
     ]
    }
   ],
   "source": [
    "# check number of duplicates\n",
    "num_duplicates = df.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicates:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# convert to dataframe\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates (again): 0\n"
     ]
    }
   ],
   "source": [
    "# check number of duplicates\n",
    "num_duplicates2 = df.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicates (again):\", num_duplicates2)\n",
    "# print(df.shape[0],df.head())\n",
    "df.head();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save the clean dataset into an sqlite database.\n",
    "You can do this with pandas [`to_sql` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html) combined with the SQLAlchemy library. Remember to import SQLAlchemy's `create_engine` in the first cell of this notebook to use it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an SQLAlchemy engine to connect to the database\n",
    "engine = create_engine('sqlite:///MessagesDump.db')\n",
    "\n",
    "# Write the DataFrame to a SQL database table named 'D_Messages'\n",
    "df.to_sql('D_Messages', engine, index=False, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Use this notebook to complete `etl_pipeline.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database based on new datasets specified by the user. Alternatively, you can complete `etl_pipeline.py` in the classroom on the `Project Workspace IDE` coming later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
